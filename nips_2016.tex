\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

\usepackage{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[]{algorithm2e}
\usepackage[usenames, dvipsnames]{color}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}

\title{LSHTS:~A Fast Time Series Similarity Search Framework using Locality Sensitive Hashing}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Chen~Luo\thanks{Use footnote for providing further
    information about author (webpage, alternative
    address)---\emph{not} for acknowledging funding agencies.} \\
  Department of Computer Science\\
  Rice University\\
  Houston, Texas \\
  \texttt{cl67@rice.edu} \\
  %% examples of more authors
  \And
  Anshumali Shrivastava \\
  Rice University \\
  Houston, Texas \\
  \texttt{anshumali@rice.edu} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
Similarity search problem on time series, as an important machine learning task, has to be fast and accuracy. Accuracy can be achieved by choosing right similarity metrics. 
Dynamic time warping can deal with the time warping of time series, and is demonstrated the most effective similarity metrics for time series. However, due to the high complexity of dynamic time warping, fast similarity search using dynamic time warping is still and open problem. 

Locality sensitive hashing (LSH) is demonstrated very effective in dealing with fast nearest neighbor search in sub-linear time. 
So, in this paper, we design LSHTS, a time series search framework to make the time series similarity search sub-linear. 
LSHTS can do fast nearest neighbor search in sub-linear time.
The experiment on large time series benchmarks demonstrated the effectiveness and efficiency of LSHTS.

\end{abstract}

\section{Introduction}

The focus of this paper is on the problem of similarity search on time series data.
Given a time series $X$, which is defined as a sequence of values $X = \{x_1,x_2,...,x_m\}$, associated with timestamps:$\{t(x_1), t(x_2),..., t(x_m)\}$ that typically have the relationship of $t(x_i) = t(x_{i-1})+\tau$, where $\tau$ is  the sampling interval, and $m$ is the number of points in the time series. Formally, we interested in efficiently computing

\begin{equation}
X^* = \arg \max_{X \in D} S(X_q,X)
\end{equation}
where $S(X,Y)$ denote the similarity between time series $X$ and $Y$. 

Similarity search problem in time series data is ubiquitous in data driven applications including robotics,medicine~\cite{oates2000method,caracca2000discovering}, speech~\cite{rabiner1993fundamentals}, object detection in vision~\cite{yang2002detecting, sonka2014image}, High Performance Computing (HPC) and system failure diagnosis~\cite{luo2014correlating,sun2014querying}, Earth Science \cite{mudelsee2013climate}, and Finance \cite{granger2014forecasting}, etc. 
{\color{Red} NEED TO ADD MORE APPLICATIONS HERE, CAN HAVE A LIST OF APPLICATIONS}

Similarity search problem, as an important machine learning task, has to be fast and accuracy \cite{kale2014examination}. Accuracy can be achieved by choosing right similarity metrics. Dynamic time warping \cite{rakthanmanon2012searching} is demonstrated as the best time series similarity method. However, because of the high complexity of dynamic time warping (DTW), fast similarity search using dynamic time warping is still and open problem. In \cite{rakthanmanon2012searching}, the authors introduces several time series sub-sequence search pruning strategies: the UCR Suite (including early abandoning strategies, lower bound, indexing, computation-reuse). The UCR suite can effectively is demonstrated very well in real world problems. UCR suite.
However, DTW distance is quite high data dependency. so when dealing with very long time series queries, it still need to take long time, and also thoretically, the worst case of UCR suite similarity is still linear $O(n)$. As a result, 
So, in this paper, we want to design a time series search framework to make the time series similarity search in sub-linear.

We apply locality-sensitive hashing schema \cite{gionis1999similarity} to the problem of fast search for time series similarity search. In this paper, we design LSHTS, a time series similarity search framework using locality sensitive hashing schema. 
In LSHTS, we combine both the UCR suit strategies (early abandoning strategies, lower bound, indexing, computation-reuse) and locality sensitive hashing schema to make time series searching much faster. The experiment on two large scale time series data shows the effectiveness and efficiency of LSHTS.
{\color{Red} MODIFY HERE TO INTRODUCE MORE NUMBERS HERE}

\subsection{Our Contributions}
{\color{Red} MAKE CONTRIBUTION MORE DETAIL.}

\begin{enumerate}
  \item We propose LSHTS, a fast time series similarity search framework using locality sensitive hashing.
  
  \item LSHTS combines both the UCR suit strategies (early abandoning strategies, lower bound, indexing, computation-reuse) and locality sensitive hashing schema to make time series searching much faster.
  
  \item The experimental result on two real world large data set (million level) shows the effectiveness and efficiency of our framework.
\end{enumerate}



\section{Background}
Show the background of our method.

\subsection{Notation and Definition}

In this section, we introduce some notions of time series. 

A time series is defined as follow:
\begin{definition}[Time Series]
A time series, denoted as $X = (x_1,x_2,...,x_m)$, where $m$ is the number of points in the time series. The timestamps of a time series, denoted as $TX = (t(x1), t(x2),..., t(xn))$, have the relationship of $t(x_i) = t(x_{i-1})+\tau$, where $\tau$is the sampling interval.
\end{definition}

In this paper, we interested in the problem of time series similarity search problem:
\begin{definition}[Top $1$ Similarity Search]
Given a data set $\{X_i|0<i<N-1\}$, where $N$ denotes the size of this data set. Given a query time series $X_q$, the top $1$ similarity search is to find the a time series $X^* \in D$, where

\[
X^* = \arg \max_{X \in D} S(X_q,X)
\]
where $S(X,Y)$ denote the similarity between time series $X$ and $Y$. We introduce the similarity measures in Section \ref{relatedwork}. In this work, we are more interested in the problem of top-k similarity search problem, which is finding the top $k$ most similar time series to $X_q$ in $D$. It is pointed out that, the size $N$ of data set $D$ often huge (i.e. million level). And in this paper, our aim is to do fast searching top $k$ time series.

\end{definition}

\subsection{Related Works}
\label{relatedwork}

First paragraph introduce the similarity between time series data.
Dynamic time warping \cite{rakthanmanon2012searching} time series is regarded as the most widely used similarity method between time series. 

Second paragraph introduce the some top-k similarity search related works in this area.

Third paragraph introduce the LSH methods.


\section{LSHTS Framework}
In this section, we introduce the LSHTS framework in details. Our framework two steps. First step is to transfer a time series into a set using sliding and n-gram.
Then using weghted min-hash to do fast nearest neighbor search.

\subsection {Time Series sliding}

\subsection {N-gram}

\subsection {Weighted Minhash}

\begin{algorithm}[h]
	\KwData{Vector $x$, and random seed[][]}
	\KwResult{Hash Pairs}
	initialization $i=0$\;
	\For{$i$ to k}{
		\For{Iteration over $x_i$}
		{
			random seeds = seed[i][j]\;
			sample $r_{i,j}$ and $c_{i,j}$ \~ Gamma(2,1)\;
			sample $\beta_{i,j}$ \~ Gamma(0,1)\;
			$t_j=|\frac{\log x_i}{r_{i,j}}+\beta_{i,j}|$\;
			$y_i=exp(r_{i,j}(t_j-beta_{i,j}))$\;
			$z_j=y_i*exp(r_{i,j})$\;
			$a_j=c_{i,j}/z_j$
			
		}
		$K^* = \arg\min_ja_i$\;
		HashPairs[i]=($k^*,t_k^*$)
	}
	\caption{Weighted Minhash (CWS)}
\end{algorithm}

For more details of this algorithm, please refer to paper. \cite{ioffe2010improved}. 

\subsection{Overall Framework}

\section{Evaluation}

\subsection{Dataset}

%\begin{figure}[h]
%  \centering
%  \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
%  \caption{Sample figure caption.}
%\end{figure}
%
%\subsection{Tables}
%
%\begin{table}[t]
%  \caption{Sample table title}
%  \label{sample-table}
%  \centering
%  \begin{tabular}{lll}
%    \toprule
%    \multicolumn{2}{c}{Part}                   \\
%    \cmidrule{1-2}
%    Name     & Description     & Size ($\mu$m) \\
%    \midrule
%    Dendrite & Input terminal  & $\sim$100     \\
%    Axon     & Output terminal & $\sim$10      \\
%    Soma     & Cell body       & up to $10^6$  \\
%    \bottomrule
%  \end{tabular}
%\end{table}

\subsubsection*{Acknowledgments}

We are thankful for...

\section*{References}
\small

\bibliographystyle{abbrv}
\bibliography{nips_2016}

%[1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms
%for connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and
%T.K.\ Leen (eds.), {\it Advances in Neural Information Processing
%  Systems 7}, pp.\ 609--616. Cambridge, MA: MIT Press.
%
%[2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS:
%  Exploring Realistic Neural Models with the GEneral NEural SImulation
%  System.}  New York: TELOS/Springer--Verlag.
%
%[3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of
%learning and recall at excitatory recurrent synapses and cholinergic
%modulation in rat hippocampal region CA3. {\it Journal of
%  Neuroscience} {\bf 15}(7):5249-5262.

\end{document}
