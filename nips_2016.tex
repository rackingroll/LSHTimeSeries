\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

\usepackage{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage[usenames, dvipsnames]{color}
\usepackage{amsmath}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\title{LSHTS:~A Fast Time Series Similarity Search Framework using Locality Sensitive Hashing}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Chen~Luo\thanks{Use footnote for providing further
    information about author (webpage, alternative
    address)---\emph{not} for acknowledging funding agencies.} \\
  Department of Computer Science\\
  Rice University\\
  Houston, Texas \\
  \texttt{cl67@rice.edu} \\
  %% examples of more authors
  \And
  Anshumali Shrivastava \\
  Rice University \\
  Houston, Texas \\
  \texttt{anshumali@rice.edu} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
Similarity search problem on time series, as an important machine learning task, has to be fast and accuracy. Accuracy can be achieved by choosing right similarity metrics. 
Dynamic time warping can deal with the time warping of time series, and is demonstrated the most effective similarity metrics for time series. However, due to the high complexity of dynamic time warping, fast similarity search using dynamic time warping is still and open problem. 

Locality sensitive hashing (LSH) is demonstrated very effective in dealing with fast nearest neighbor search in sub-linear time. 
So, in this paper, we design LSHTS, a time series search framework to make the time series similarity search sub-linear. 
LSHTS can do fast nearest neighbor search in sub-linear time.
The experiment on large time series benchmarks demonstrated the effectiveness and efficiency of LSHTS.

\end{abstract}

\section{Introduction}

The focus of this paper is on the problem of similarity search on time series data.
Given a time series $X$, which is defined as a sequence of values $X = \{x_1,x_2,...,x_m\}$, associated with timestamps:$\{t(x_1), t(x_2),..., t(x_m)\}$ that typically have the relationship of $t(x_i) = t(x_{i-1})+\tau$, where $\tau$ is  the sampling interval, and $m$ is the number of points in the time series. Formally, we interested in efficiently computing

\begin{equation}
X^* = \arg \max_{X \in D} S(X_q,X)
\end{equation}
where $S(X,Y)$ denote the similarity between time series $X$ and $Y$. 

Similarity search problem in time series data is ubiquitous in data driven applications including robotics,medicine~\cite{oates2000method,caracca2000discovering}, speech~\cite{rabiner1993fundamentals}, object detection in vision~\cite{yang2002detecting, sonka2014image}, High Performance Computing (HPC) and system failure diagnosis~\cite{luo2014correlating,sun2014querying}, Earth Science \cite{mudelsee2013climate}, and Finance \cite{granger2014forecasting}, etc. 
{\color{Red} NEED TO ADD MORE APPLICATIONS HERE, CAN HAVE A LIST OF APPLICATIONS}

Similarity search problem, as an important machine learning task, has to be fast and accuracy \cite{kale2014examination}. Accuracy can be achieved by choosing right similarity metrics. Dynamic time warping \cite{rakthanmanon2012searching} is demonstrated as the best time series similarity method. However, because of the high complexity of dynamic time warping (DTW), fast similarity search using dynamic time warping is still and open problem. In \cite{rakthanmanon2012searching}, the authors introduces several time series sub-sequence search pruning strategies: the UCR Suite (including early abandoning strategies, lower bound, indexing, computation-reuse). The UCR suite can effectively is demonstrated very well in real world problems. UCR suite.
However, DTW distance is quite high data dependency. so when dealing with very long time series queries, it still need to take long time, and also thoretically, the worst case of UCR suite similarity is still linear $O(n)$. As a result, 
So, in this paper, we want to design a time series search framework to make the time series similarity search in sub-linear.

We apply locality-sensitive hashing schema \cite{gionis1999similarity} to the problem of fast search for time series similarity search. In this paper, we design LSHTS, a time series similarity search framework using locality sensitive hashing schema. 
In LSHTS, we combine both the UCR suit strategies (early abandoning strategies, lower bound, indexing, computation-reuse) and locality sensitive hashing schema to make time series searching much faster. The experiment on two large scale time series data shows the effectiveness and efficiency of LSHTS.
{\color{Red} MODIFY HERE TO INTRODUCE MORE NUMBERS HERE}

\subsection{Our Contributions}
{\color{Red} MAKE CONTRIBUTION MORE DETAIL.}

\begin{enumerate}
  \item We propose LSHTS, a fast time series similarity search framework using locality sensitive hashing.
  
  \item LSHTS combines both the UCR suit strategies (early abandoning strategies, lower bound, indexing, computation-reuse) and locality sensitive hashing schema to make time series searching much faster.
  
  \item The experimental result on two real world large data set (million level) shows the effectiveness and efficiency of our framework.
\end{enumerate}

\section{Background}
Show the background of our method.

\subsection{Notation and Definition}

In this section, we introduce some notions of time series. 

A time series is defined as follow:
\begin{definition}[Time Series]
A time series, denoted as $X = (x_1,x_2,...,x_m)$, where $m$ is the number of points in the time series. The timestamps of a time series, denoted as $TX = (t(x1), t(x2),..., t(xn))$, have the relationship of $t(x_i) = t(x_{i-1})+\tau$, where $\tau$is the sampling interval.
\end{definition}

In this paper, we interested in the problem of time series similarity search problem:
\begin{definition}[Top $1$ Similarity Search]
Given a data set $\{X_i|0<i<N-1\}$, where $N$ denotes the size of this data set. Given a query time series $X_q$, the top $1$ similarity search is to find the a time series $X^* \in D$, where

\begin{equation}
\label{knn}
X^* = \arg \max_{X \in D} S(X_q,X)
\end{equation}


where $S(X,Y)$ denote the similarity between time series $X$ and $Y$. We introduce the similarity measures in Section \ref{relatedwork}. In this work, we are more interested in the problem of top-k similarity search problem, which is finding the top $k$ most similar time series to $X_q$ in $D$. It is pointed out that, the size $N$ of data set $D$ often huge (i.e. million level). And in this paper, our aim is to do fast searching top $k$ time series.

\end{definition}

\section{Related Works}
\label{relatedwork}

First paragraph introduce the similarity between time series data.
Dynamic time warping \cite{rakthanmanon2012searching} time series is regarded as the most widely used similarity method between time series. 

Second paragraph introduce the some top-k similarity search related works in this area.

Third paragraph introduce the LSH methods.


\section{LSHTS Framework}

In this section, we introduce LSHTS framework for fast similarity search in time series data \ref{knn}. 


. Our framework two steps. First step is to transfer a time series into a set using sliding and n-gram.
Then using weghted min-hash to do fast nearest neighbor search.

\subsection {Time Series sliding}

\subsection {N-gram}

\subsection {Weighted Minhash}

\begin{algorithm}
\caption{My algorithm}\label{euclid}
\begin{algorithmic}[1]
\Procedure{MyProcedure}{}
\State $\textit{stringlen} \gets \text{length of }\textit{string}$
\State $i \gets \textit{patlen}$
\BState \emph{top}:
\If {$i > \textit{stringlen}$} \Return false
\EndIf
\State $j \gets \textit{patlen}$
\BState \emph{loop}:
\If {$\textit{string}(i) = \textit{path}(j)$}
\State $j \gets j-1$.
\State $i \gets i-1$.
\State \textbf{goto} \emph{loop}.
\State \textbf{close};
\EndIf
\State $i \gets i+\max(\textit{delta}_1(\textit{string}(i)),\textit{delta}_2(j))$.
\State \textbf{goto} \emph{top}.
\EndProcedure
\end{algorithmic}
\end{algorithm}

%\begin{algorithm}[h]
%	\KwData{Vector $x$, and random seed[][]}
%	\KwResult{Hash Pairs}
%	initialization $i=0$\;
%	\For{$i$ to k}{
%		\For{Iteration over $x_i$}
%		{
%			random seeds = seed[i][j]\;
%			sample $r_{i,j}$ and $c_{i,j}$ \~ Gamma(2,1)\;
%			sample $\beta_{i,j}$ \~ Gamma(0,1)\;
%			$t_j=|\frac{\log x_i}{r_{i,j}}+\beta_{i,j}|$\;
%			$y_i=exp(r_{i,j}(t_j-beta_{i,j}))$\;
%			$z_j=y_i*exp(r_{i,j})$\;
%			$a_j=c_{i,j}/z_j$
%			
%		}
%		$K^* = \arg\min_ja_i$\;
%		HashPairs[i]=($k^*,t_k^*$)
%	}
%	\caption{Weighted Minhash (CWS)}
%\end{algorithm}

For more details of this algorithm, please refer to paper. \cite{ioffe2010improved}. 

\subsection{Overall Framework}

\section{Experiment}
In this section, test our framework on two different type of dataset. Our experiments aims to address the following three questions:
\begin{enumerate}
  \item [RQ1:] How accuracy does LSHTS do compared with the exact search result. 
  \item [RQ2:] How efficient can LSHTS achieve.
\end{enumerate}

\subsection{Dataset}

In order to test both the effective and efficiency of our algorithm, we choose two large time series data used in \cite{rakthanmanon2012searching}: Random Walk, and one Day ECG \footnote{http://www.cs.ucr.edu/~eamonn/UCRsuite.html}.

Random Walks models the similarity search schema very well, it often used for testing the similarity search methods \cite{rakthanmanon2012searching}. 
In this paper, we use the random walks data set provided by UCR suite. This data set contains $20$ million data point. In \cite{rakthanmanon2012searching}, the authors searches the all the sub-sequence of the time series. So, in this paper, we transfer this data set into a time series similarity search data set. We generate all the sub-series in that data set, then the final data set consist $20$ million time series.

The second data set we use is the 22 hours and 23 minutes of ECG data (20,140,000 datapoints) together with the exact query. This data set also comes from the UCR suite time series data set. The same as Random walks data set, we generate all the sub-series for this (20,140,000 datapoints time series), and search all the sub-series.

\subsection{Effectiveness Study}
In this section, we test LSHTS in terms of accuracy compared with the exact search result obtained by DTW distance.

The result is showed bellow:

\begin{table}[t]
  \caption{Top k search accuracy on ECG data set}
  \label{accuracy_ecg}
  \centering
  \begin{tabular}{lllll}
    \toprule
    Time Series Length     & 128     & 512  & 1024  & 2048\\
    \midrule
    Top@5 & 100\%  & 100\% & 100\% & 100\%     \\
    Top@10 & 90\%  & 100\% & 100\% & 100\%     \\
    Top@20 & 95\%  & 90\% & 95\% & 95\%     \\
    Top@20 & 90\%  & 88\% & 92\% & 94\%     \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[t]
  \caption{Top k search accuracy on random walk data set}
  \label{accuracy_random}
  \centering
  \begin{tabular}{lllll}
    \toprule
    Time Series Length     & 128     & 512  & 1024  & 2048\\
    \midrule
    Top@5 & 100\%  & 100\% & 100\% & 100\%     \\
    Top@10 & 100\%  & 100\% & 100\% & 100\%     \\
    Top@20 & 95\%  & 95\% & 90\% & 95\%     \\
    Top@20 & 88\%  & 88\% & 90\% & 92\%     \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Efficiency Study}

In this section, we introduce the experiment result in terms of running time compared with the UCR suite methods. This result is showed in Table. \ref{time}

\begin{table}[t]
  \caption{CPU Execution time for different methods}
  \label{time}
  \centering
  \begin{tabular}{lllll}
    \toprule
    Time Series Length     & 128     & 512  & 1024  & 2048\\
    \midrule
    Exact Search (UCR Suite) & 100\%  & 100\% & 100\% & 100\%     \\
    LSHTS & 100\%  & 100\% & 100\% & 100\%     \\
    \bottomrule
  \end{tabular}
\end{table}

\subsubsection*{Acknowledgments}

We are thankful for...

\section*{References}
\small

\bibliographystyle{abbrv}
\bibliography{nips_2016}

%[1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms
%for connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and
%T.K.\ Leen (eds.), {\it Advances in Neural Information Processing
%  Systems 7}, pp.\ 609--616. Cambridge, MA: MIT Press.
%
%[2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS:
%  Exploring Realistic Neural Models with the GEneral NEural SImulation
%  System.}  New York: TELOS/Springer--Verlag.
%
%[3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of
%learning and recall at excitatory recurrent synapses and cholinergic
%modulation in rat hippocampal region CA3. {\it Journal of
%  Neuroscience} {\bf 15}(7):5249-5262.

\end{document}
